{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dc43ea",
   "metadata": {
    "id": "23dc43ea"
   },
   "source": [
    "# Land use and Land Cover Classification\n",
    "\n",
    "The availability of free satellite data has increased its use in several applications in the domains of agriculture, disaster recovery, climate change, urban development, or environmental monitoring can be realized. However, to fully utilize the data for the previously mentioned domains, first satellite images must be processed and transformed into structured semantics. One type of such fundamental semantics is Land Use and Land Cover Classification. The aim of land use and land cover classification is to automatically provide labels describing the represented physical land type or how a land area is used (e.g., residential, industrial)\n",
    "\n",
    "A satellite image dataset for the task of land use and land cover classification was presented in [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8519248). The EuroSAT dataset is based on Sentinel-2 satellite images and consists of 27,000 labeled images with a total of 10 different classes listed below where the patches are 64x64 pixels each.\n",
    "\n",
    "![alt text](./Images/dataset.png \"The EuroSAT Dataset\")\n",
    "\n",
    "In this assignment you are going to use the optical bands of Sentinel-2 which are computed by combining the bands red (B04), green (B03) and blue (B02) from the Sentinel-2 product. More information about the Sentinel-2 bands can be found [here](https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial). You will then train different Convolutional Neural Network (CNN) models to classify every 64x64 patches in one of the following classes:\n",
    "\n",
    "1. AnnualCrop\n",
    "2. Forest\n",
    "3. Herbaceous Vegetation\n",
    "4. Highway\n",
    "5. Industrial\n",
    "6. Pasture\n",
    "7. Permanent Crop\n",
    "8. Residential\n",
    "9. River\n",
    "10. SeaLake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122de36",
   "metadata": {
    "id": "c122de36"
   },
   "source": [
    "## Instructions\n",
    "The EuroSAT dataset is based on Sentinel-2 satellite images and consists of 27,000 labeled images with a total of 10 different classes. The dataset is structured as follows:\n",
    "1. `train.txt`: this file contains a list of images that will be used to train the Convolutional Neural Network (CNN) models.\n",
    "2. `test.txt`: this file contains a list of images that will be used to test the Convolutional Neural Network (CNN) models.\n",
    "3. A list of 10 folders, each one containing the images pertaining to each class.\n",
    "\n",
    "All code needs to be developed in Python 3 and run on a Ubuntu 20.04 environment or later versions of Ubuntu. The student is requested send the jupyter notebook using the template provided. Any textual or visual information and equations that the student might need to convey is expected to be written using the markdown language within the same Juputer Notebook. The Juputer Notebook should be named as follows\n",
    "\n",
    "`name-surname.ipynb`\n",
    "\n",
    "The list of packages that are allowed for this assignment are: `matplotlib`, `os`, `numpy`, `torch`, `open-cv`, `torchvision` and any other packages agreed with the lecturer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f06d9",
   "metadata": {
    "id": "f58f06d9"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "**Q1:** A lot of effort in solving any machine learning and computer vision problem goes into preparing the data. PyTorch provides a simple mechanism to define a custom dataset using `torch.utils.data.Dataset`, which is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n",
    "\n",
    "- `__init__` so that it initializes the dataset\n",
    "- `__len__` so that len(dataset) returns the size of the dataset.\n",
    "- `__getitem__` to support the indexing such that dataset[i] can be used to get i-th sample.\n",
    "\n",
    "\n",
    "Write a class `DataLoaderClassification` that can be used to\n",
    "- load the list of image filenames and the corresponding lables in two lists in `__init__`\n",
    "- load a batch of images and corresponding lables when one calls `__getitem__`\n",
    "- returns the length of the dataset using `__len__`\n",
    "\n",
    "Write the code in one or more cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b4ff6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "Debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68f7118b",
   "metadata": {
    "id": "68f7118b"
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "#  DataLoaderClassification() ::              #\n",
    "###############################################\n",
    "class DataLoaderClassification(Dataset):\n",
    "    def __init__(self, data_dir, file_list_path):\n",
    "        # Args:\n",
    "        #    data_dir :: Path to the directory containing image subfolders.\n",
    "        #    file_list_path :: Path to the text file containing image paths (e.g., train.txt or test.txt).\n",
    "\n",
    "        self.data_dir = data_dir        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # A set to collect unique labels\n",
    "        label_set = set()\n",
    "        \n",
    "        with open(file_list_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    updated_path = line.replace('EuroSAT/', '')  \n",
    "                    self.image_paths.append(updated_path)\n",
    "                    \n",
    "                    label = os.path.split(updated_path)[0].replace('./Data/', '')  \n",
    "                    self.labels.append(label)\n",
    "                    label_set.add(label)  # Collect unique labels\n",
    "                    \n",
    "        # Create a mapping from label strings to integers\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_full_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = cv2.imread(img_full_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image at path {img_full_path} could not be loaded.\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)# Convert to RGB\n",
    "\n",
    "        # image to tensor\n",
    "        transform = transforms.ToTensor()\n",
    "        image = transform(image)\n",
    "        \n",
    "        # Convert label from string to integer using the label mapping and, then, to tensor\n",
    "        label = self.label_to_index[label]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52533b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train_dataset: 21600\n",
      "Number of items in test_dataset: 5400\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'Data'\n",
    "train_file_list = 'Data/train.txt'\n",
    "test_file_list = 'Data/test.txt'\n",
    "\n",
    "train_dataset = DataLoaderClassification(data_dir=data_dir, file_list_path=train_file_list)\n",
    "test_dataset  = DataLoaderClassification(data_dir=data_dir, file_list_path=test_file_list)\n",
    "\n",
    "print(f\"Number of items in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of items in test_dataset: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "# test for getitem\n",
    "if Debug:\n",
    "    test_index = random.randint(0, 21599)\n",
    "    print(f\"Check item {test_index} from train_dataset\")\n",
    "    image, label = train_dataset[test_index]\n",
    "    print(f\"Image {test_index} -> Shape: {image.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9eb1c0",
   "metadata": {
    "id": "8f9eb1c0"
   },
   "source": [
    "**Q2:** Write the code in one cell that uses the list of files included in `train.txt` and `test.txt` to create a Pytorch dataloader for the training and testing data, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f76928",
   "metadata": {},
   "source": [
    "**Loading in batches**\n",
    "\n",
    "DataLoader: Shuffling while training to prevent learning parterns related to the order.\n",
    "When testing, shuffling is typically set to False to maintain the order of the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a5aad63",
   "metadata": {
    "id": "9a5aad63"
   },
   "outputs": [],
   "source": [
    "#num_workers = 0   # number of subprocesses to use for data loading\n",
    "batch_size = 20   # how many samples per batch to load\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test: Iterate through the dataset in batches\n",
    "if Debug:\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        print(f\"\\n Batch {batch_idx + 1}  \\n   Image batch shape: {images.shape}, \\n   Labels: {labels}\")\n",
    "        if batch_idx == 2:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b3aa2",
   "metadata": {
    "id": "e11b3aa2"
   },
   "source": [
    "**Q3:** PyTorch provides the elegantly designed modules and classes, including `torch.nn`, to help you create and train neural networks. An `nn.Module` contains layers, and a method `forward(input)` that returns the output. Write the `CNN` class to define a Convolutional Neural Network (CNN) where the first convolutional layer (`conv1`) takes 3 input channels, outputs 16 output channels and has a kernel size of 5. The output of `conv1` is fed into a ReLU followed by a Max-pooling operator. The second convolutional layer in this network (`conv2`) should have 32 filters with a kernel size of 5 followed by a ReLU and a max-pooling operator. The last layer is a fully-connected layer (`fc1`) with 10 output neurons. In this code you should define the `__init__` and `forward` member functions.\n",
    "\n",
    "More information about `torch.nn` can be found [here](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79aa9c30",
   "metadata": {
    "id": "79aa9c30"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "###############################################\n",
    "#  SimpleCNN() :: Arquitecture defined in Q3 #\n",
    "###############################################\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=32 * 13 * 13, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "                                                     # --> 64x64x3 -->\n",
    "        x = F.relu(self.conv1(x))                    # --> 16x60x60 (kernel_size=5 without padding)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # --> 16x30x30 \n",
    "        x = F.relu(self.conv2(x))                    # --> 32x26x26 (kernel_size=5 without padding)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # --> 32x13x13\n",
    "        x = x.view(-1, 32 * 13 * 13)                 # --> 1x5408\n",
    "        x = self.fc1(x)                              # --> 1x10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926b22b",
   "metadata": {},
   "source": [
    "Pooling with kernel size of 2 and stride of 2 (typicall choice) -> downsamples the feature map by a factor of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b64c8",
   "metadata": {
    "id": "251b64c8"
   },
   "source": [
    "**Q4:** Write the code in one or more cells to train the CNN specified in **Q3**. Plot the accuracy against the number of epochs.  Save the best performing model in the folder `./Model/Simple-CNN/model.pth` and print the highest accuracy achieved after 100 epochs using the markdown language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9c921e2",
   "metadata": {
    "id": "b9c921e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n",
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=5408, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available() # I have an M3, so no cuda for me \n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95b248c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# split train in validation + train\n",
    "valid_size = 0.2  # percentage of training set to use as validation\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "# Debug\n",
    "if Debug:\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        print(f\"\\n Train batch {batch_idx + 1}  \\n   Image batch shape: {images.shape}, \\n   Labels: {labels}\")\n",
    "        if batch_idx == 2:\n",
    "            break\n",
    "    for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "        print(f\"\\n Validation batch {batch_idx + 1}  \\n   Image batch shape: {images.shape}, \\n   Labels: {labels}\")\n",
    "        if batch_idx == 2:\n",
    "            break\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "valid_loss_min = np.Inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c79ef2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# RUN TRAINING\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m train_losses, valid_losses, train_accuracies, valid_accuracies \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39mn_epochs)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Plot accuracy and loss\u001b[39;00m\n\u001b[1;32m    120\u001b[0m plot_accuracy(train_accuracies, valid_accuracies, num_epochs\u001b[38;5;241m=\u001b[39mn_epochs)\n",
      "Cell \u001b[0;32mIn[50], line 74\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs, save_path, save_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct_train \u001b[38;5;241m/\u001b[39m total_train\n\u001b[1;32m     72\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct_valid \u001b[38;5;241m/\u001b[39m total_valid\n\u001b[0;32m---> 74\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n\u001b[1;32m     75\u001b[0m valid_accuracies\u001b[38;5;241m.\u001b[39mappend(valid_accuracy)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Print training/validation statistics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################################\n",
    "#  train_model() :: Training function #\n",
    "#######################################\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, save_path=\"./Model/Simple-CNN/\", save_name =\"model.pth\"):\n",
    "    \n",
    "    # Keep track of training and validation loss\n",
    "    best_accuracy = 0.0\n",
    "    valid_loss_min = np.Inf  # Initialize minimum validation loss\n",
    "    train_losses, valid_losses = [], []          # Store loss values for plotting\n",
    "    train_accuracies, valid_accuracies = [], []  # Store accuracy values for plotting\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        ###################\n",
    "        # train the model # --> train_loader\n",
    "        ###################\n",
    "        model.train()  # Set the model to training mode\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()               # Clear the gradients\n",
    "            output = model(images)              # Forward pass, calculate ouput\n",
    "            loss = criterion(output, labels)    # Calculate the loss\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)  # Track training loss\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)  # Get predicted class\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        ######################\n",
    "        # validate the model # --> valid_loader\n",
    "        ######################\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for images, labels in valid_loader:\n",
    "                if train_on_gpu:\n",
    "                    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "                output = model(images)                      # Forward pass\n",
    "                loss = criterion(output, labels)            # Calculate the validation loss\n",
    "                valid_loss += loss.item() * images.size(0)  # Track validation loss\n",
    "\n",
    "                _, predicted = torch.max(output, 1)         # Get predicted class\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Calculate training and validation accuracy\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        valid_accuracy = 100 * correct_valid / total_valid\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        # Print training/validation statistics\n",
    "        print(f'Epoch: {epoch} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f} \\tTraining Accuracy: {train_accuracy:.2f}% \\tValidation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "        # Save the model if validation loss has decreased\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        save_full_path = os.path.join(save_path, save_name)\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model ...')\n",
    "            torch.save(model.state_dict(), save_full_path)\n",
    "            valid_loss_min = valid_loss  # Update minimum validation loss\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
    "\n",
    "\n",
    "# Plot accuracy vs epochs\n",
    "def plot_accuracy(train_accuracies, valid_accuracies, num_epochs):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, train_accuracies, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, valid_accuracies, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Plot loss vs epochs\n",
    "def plot_loss(train_losses, valid_losses, num_epochs):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, train_losses, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, valid_losses, 'b', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# RUN TRAINING\n",
    "train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=n_epochs)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plot_accuracy(train_accuracies, valid_accuracies, num_epochs=n_epochs)\n",
    "plot_loss(train_losses, valid_losses, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d51492",
   "metadata": {
    "id": "82d51492"
   },
   "source": [
    "**Q5:** Your role as a researcher is to improve the performance of the current neural network. Explain the architecture that provided the best performance and describe the modifications that you think provided the gain.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283f982",
   "metadata": {
    "id": "9283f982"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0735709",
   "metadata": {
    "id": "b0735709"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
