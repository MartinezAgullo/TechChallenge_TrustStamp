{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dc43ea",
   "metadata": {
    "id": "23dc43ea"
   },
   "source": [
    "# Land use and Land Cover Classification\n",
    "\n",
    "The availability of free satellite data has increased its use in several applications in the domains of agriculture, disaster recovery, climate change, urban development, or environmental monitoring can be realized. However, to fully utilize the data for the previously mentioned domains, first satellite images must be processed and transformed into structured semantics. One type of such fundamental semantics is Land Use and Land Cover Classification. The aim of land use and land cover classification is to automatically provide labels describing the represented physical land type or how a land area is used (e.g., residential, industrial)\n",
    "\n",
    "A satellite image dataset for the task of land use and land cover classification was presented in [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8519248). The EuroSAT dataset is based on Sentinel-2 satellite images and consists of 27,000 labeled images with a total of 10 different classes listed below where the patches are 64x64 pixels each.\n",
    "\n",
    "![alt text](./Images/dataset.png \"The EuroSAT Dataset\")\n",
    "\n",
    "In this assignment you are going to use the optical bands of Sentinel-2 which are computed by combining the bands red (B04), green (B03) and blue (B02) from the Sentinel-2 product. More information about the Sentinel-2 bands can be found [here](https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial). You will then train different Convolutional Neural Network (CNN) models to classify every 64x64 patches in one of the following classes:\n",
    "\n",
    "1. AnnualCrop\n",
    "2. Forest\n",
    "3. Herbaceous Vegetation\n",
    "4. Highway\n",
    "5. Industrial\n",
    "6. Pasture\n",
    "7. Permanent Crop\n",
    "8. Residential\n",
    "9. River\n",
    "10. SeaLake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122de36",
   "metadata": {
    "id": "c122de36"
   },
   "source": [
    "## Instructions\n",
    "The EuroSAT dataset is based on Sentinel-2 satellite images and consists of 27,000 labeled images with a total of 10 different classes. The dataset is structured as follows:\n",
    "1. `train.txt`: this file contains a list of images that will be used to train the Convolutional Neural Network (CNN) models.\n",
    "2. `test.txt`: this file contains a list of images that will be used to test the Convolutional Neural Network (CNN) models.\n",
    "3. A list of 10 folders, each one containing the images pertaining to each class.\n",
    "\n",
    "All code needs to be developed in Python 3 and run on a Ubuntu 20.04 environment or later versions of Ubuntu. The student is requested send the jupyter notebook using the template provided. Any textual or visual information and equations that the student might need to convey is expected to be written using the markdown language within the same Juputer Notebook. The Juputer Notebook should be named as follows\n",
    "\n",
    "`name-surname.ipynb`\n",
    "\n",
    "The list of packages that are allowed for this assignment are: `matplotlib`, `os`, `numpy`, `torch`, `open-cv`, `torchvision` and any other packages agreed with the lecturer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f06d9",
   "metadata": {
    "id": "f58f06d9"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "**Q1:** A lot of effort in solving any machine learning and computer vision problem goes into preparing the data. PyTorch provides a simple mechanism to define a custom dataset using `torch.utils.data.Dataset`, which is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n",
    "\n",
    "- `__init__` so that it initializes the dataset\n",
    "- `__len__` so that len(dataset) returns the size of the dataset.\n",
    "- `__getitem__` to support the indexing such that dataset[i] can be used to get i-th sample.\n",
    "\n",
    "\n",
    "Write a class `DataLoaderClassification` that can be used to\n",
    "- load the list of image filenames and the corresponding lables in two lists in `__init__`\n",
    "- load a batch of images and corresponding lables when one calls `__getitem__`\n",
    "- returns the length of the dataset using `__len__`\n",
    "\n",
    "Write the code in one or more cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ece1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68f7118b",
   "metadata": {
    "id": "68f7118b"
   },
   "outputs": [],
   "source": [
    "class DataLoaderClassification(Dataset):\n",
    "    def __init__(self, data_dir, file_list_path):\n",
    "        # Args:\n",
    "        #    data_dir :: Path to the directory containing image subfolders.\n",
    "        #    file_list_path :: Path to the text file containing image paths (e.g., train.txt or test.txt).\n",
    "\n",
    "        self.data_dir = data_dir        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(file_list_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    updated_path = line.replace('EuroSAT/', '')  \n",
    "                    self.image_paths.append(updated_path)\n",
    "                    \n",
    "                    label = os.path.split(updated_path)[0].replace('./Data/', '')  \n",
    "                    self.labels.append(label)\n",
    "                    # print(f'Path: {updated_path}, label: {label}')\n",
    "        \n",
    "        # Create label to index mapping\n",
    "        #self.label_mapping = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "        #print(self.label_mapping)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_full_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        #print(f\"Loading image from: {img_full_path}\")\n",
    "        #print(f\"Label: {label}\")\n",
    "        \n",
    "        image = cv2.imread(img_full_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"pablo: Image at path {img_full_path} could not be loaded.\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        \n",
    "        # Convert label to numeric value\n",
    "        #label = self.label_mapping[label]\n",
    "        #print(f\"Label mapped: {label}\")\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e46580a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train_dataset: 21600\n",
      "Number of items in test_dataset: 5400\n",
      "Check item 9661 from train_dataset\n",
      "Image 9661 -> Shape: (64, 64, 3), Label: AnnualCrop\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'Data'\n",
    "train_file_list = 'Data/train.txt'\n",
    "test_file_list = 'Data/test.txt'\n",
    "\n",
    "train_dataset = DataLoaderClassification(data_dir=data_dir, file_list_path=train_file_list)\n",
    "test_dataset  = DataLoaderClassification(data_dir=data_dir, file_list_path=test_file_list)\n",
    "\n",
    "print(f\"Number of items in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of items in test_dataset: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "# test\n",
    "test_index = random.randint(0, 21599)\n",
    "print(f\"Check item {test_index} from train_dataset\")\n",
    "image, label = train_dataset[test_index]\n",
    "print(f\"Image {test_index} -> Shape: {image.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9eb1c0",
   "metadata": {
    "id": "8f9eb1c0"
   },
   "source": [
    "**Q2:** Write the code in one cell that uses the list of files included in `train.txt` and `test.txt` to create a Pytorch dataloader for the training and testing data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5aad63",
   "metadata": {
    "id": "9a5aad63"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b3aa2",
   "metadata": {
    "id": "e11b3aa2"
   },
   "source": [
    "**Q3:** PyTorch provides the elegantly designed modules and classes, including `torch.nn`, to help you create and train neural networks. An `nn.Module` contains layers, and a method `forward(input)` that returns the output. Write the `CNN` class to define a Convolutional Neural Network (CNN) where the first convolutional layer (`conv1`) takes 3 input channels, outputs 16 output channels and has a kernel size of 5. The output of `conv1` is fed into a ReLU followed by a Max-pooling operator. The second convolutional layer in this network (`conv2`) should have 32 filters with a kernel size of 5 followed by a ReLU and a max-pooling operator. The last layer is a fully-connected layer (`fc1`) with 10 output neurons. In this code you should define the `__init__` and `forward` member functions.\n",
    "\n",
    "More information about `torch.nn` can be found [here](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa9c30",
   "metadata": {
    "id": "79aa9c30"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b64c8",
   "metadata": {
    "id": "251b64c8"
   },
   "source": [
    "**Q4:** Write the code in one or more cells to train the CNN specified in **Q3**. Plot the accuracy against the number of epochs.  Save the best performing model in the folder `./Model/Simple-CNN/model.pth` and print the highest accuracy achieved after 100 epochs using the markdown language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c921e2",
   "metadata": {
    "id": "b9c921e2"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d51492",
   "metadata": {
    "id": "82d51492"
   },
   "source": [
    "**Q5:** Your role as a researcher is to improve the performance of the current neural network. Explain the architecture that provided the best performance and describe the modifications that you think provided the gain.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283f982",
   "metadata": {
    "id": "9283f982"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0735709",
   "metadata": {
    "id": "b0735709"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
